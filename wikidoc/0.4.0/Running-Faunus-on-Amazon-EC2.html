<!DOCTYPE html>

<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <title>Running Faunus on Amazon EC2</title>
    <link rel="stylesheet" href="./css/screen.css" type="text/css" charset="utf-8" />
    <link rel="stylesheet" href="./css/gollum.css" type="text/css" charset="utf-8" />
    <link rel="stylesheet" href="./css/syntax.css" type="text/css" charset="utf-8" />
    <script src="./javascript/jquery-1.4.2.min.js" type="text/javascript"></script>
    <script src="./javascript/jquery.text_selection-1.0.0.min.js" type="text/javascript"></script>
    <script src="./javascript/jquery.previewable_comment_form.js" type="text/javascript"></script>
    <script src="./javascript/jquery.tabs.js" type="text/javascript"></script>
    <script src="./javascript/gollum.js" type="text/javascript"></script>
  </head>

  <body>
    <div id="main">
      <div class="site">
        <div id="guides">
          <div class="guide">
            <div class="main">
              <div class="actions">
                <div>
                  <a href="./Home.html">Aurelius Faunus 0.4.0</a>
                </div>
              </div>
              <h1>Running Faunus on Amazon EC2</h1>
              <div class="content wikistyle gollum textile">
                <p><a href="http://aws.amazon.com/ec2/"><img src="http://cdn001.practicalclouds.com/user-content/1_Dave%20McCormick//logos/Amazon%20AWS%20plus%20EC2%20logo_scaled.png" alt=""></a></p>
<p><a href="http://aws.amazon.com/ec2/">Amazon EC2</a> and <a href="http://whirr.apache.org/">Whirr</a> make it easy to set up a <a href="http://hadoop.apache.org/">Hadoop</a> compute cluster that can then be utilized by Faunus. This section of documentation will explain how to set up a Hadoop cluster on Amazon EC2 and execute Faunus scripts.</p>
<h2>Setting Up Whirr</h2>
<p><span class="float-left"><span><img src="http://whirr.apache.org/images/whirr-logo.png" width="150px"></span></span></p>
<blockquote>
<p>Apache Whirr is a set of libraries for running cloud services. Whirr provides a cloud-neutral way to run services (you don’t have to worry about the idiosyncrasies of each provider), a common service <span class="caps">API</span> (the details of provisioning are particular to the service), and smart defaults for services (you can get a properly configured system running quickly, while still being able to override settings as needed). You can also use Whirr as a command line tool for deploying clusters. — <a href="http://whirr.apache.org/">The Apache Whirr Homepage</a></p>
</blockquote>
<p><br></p>
<p>Faunus provides a Whirr recipe for loading up a Hadoop cluster that is properly versioned for the Hadoop currently used by Faunus. This recipe is reproduced below. Please see the Whirr <a href="http://whirr.apache.org/docs/0.8.1/quick-start-guide.html">Quick Start</a> for more information about the parameters and how to set up an Amazon EC2 account (e.g. <code>ssh-keygen -t rsa -P ''</code> and setting <span class="caps">AWS</span> keys as environmental variables).</p>
<div class="highlight">
<pre>whirr.cluster-name<span class="o">=</span>faunuscluster
whirr.instance-templates<span class="o">=</span>1 hadoop-jobtracker+hadoop-namenode,3 hadoop-datanode+hadoop-tasktracker
whirr.provider<span class="o">=</span>aws-ec2
whirr.identity<span class="o">=</span><span class="k">${</span><span class="nv">env</span><span class="p">:</span><span class="nv">AWS_ACCESS_KEY_ID</span><span class="k">}</span>
whirr.credential<span class="o">=</span><span class="k">${</span><span class="nv">env</span><span class="p">:</span><span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="k">}</span>
whirr.private-key-file<span class="o">=</span><span class="k">${</span><span class="nv">sys</span><span class="p">:</span><span class="nv">user</span><span class="p">.home</span><span class="k">}</span>/.ssh/id_rsa
whirr.public-key-file<span class="o">=</span><span class="k">${</span><span class="nv">sys</span><span class="p">:</span><span class="nv">user</span><span class="p">.home</span><span class="k">}</span>/.ssh/id_rsa.pub
whirr.hadoop.version<span class="o">=</span>1.1.1
</pre>
</div>

<p>Once your Amazon EC2 keys and ssh key files have been properly set up, a Hadoop cluster can be launched. The recipe above creates a 4 node cluster.</p>
<div class="highlight">
<pre>faunus<span class="nv">$ </span>whirr launch-cluster --config bin/whirr.properties
Bootstrapping cluster
Configuring template
Configuring template
Starting 3 node<span class="o">(</span>s<span class="o">)</span> with roles <span class="o">[</span>hadoop-datanode, hadoop-tasktracker<span class="o">]</span>
Starting 1 node<span class="o">(</span>s<span class="o">)</span> with roles <span class="o">[</span>hadoop-namenode, hadoop-jobtracker<span class="o">]</span>
...
</pre>
</div>

<p><img src="images/ec2-screenshot.png" alt=""></p>
<p>When logging into the <a href="http://console.aws.amazon.com/ec2/">Amazon EC2 Console</a>, the cluster machines are visible. After running the Hadoop proxy shell script (in another window), the Hadoop cluster is ready for job submissions.</p>
<div class="highlight">
<pre>faunus<span class="nv">$.</span> ~/.whirr/faunuscluster/hadoop-proxy.sh
Running proxy to Hadoop cluster at ec2-23-20-32-211.compute-1.amazonaws.com. Use Ctrl-c to quit. 
</pre>
</div>

<p>A simple check to ensure that the Hadoop cluster is working is to see if <span class="caps">HDFS</span> is available.</p>
<div class="highlight">
<pre>faunus<span class="nv">$ </span><span class="nb">export </span><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span>~/.whirr/faunuscluster
faunus<span class="nv">$ </span>hadoop fs -ls /
Found 3 items
drwxr-xr-x   - hadoop supergroup          0 2012-07-20 19:13 /hadoop
drwxrwxrwx   - hadoop supergroup          0 2012-07-20 19:13 /tmp
drwxrwxrwx   - hadoop supergroup          0 2012-07-20 19:13 /user
</pre>
</div>

<h2>Recommended Map/Reduce Tasks Per Node</h2>
<p>Below is the recommended number of tasks for each <span class="caps">AWS</span> EC2 machine type. These numbers are the default values used by Amazon’s Elastic MapReduce service</p>
<div class="highlight">
<pre>m1.small 
  mapred.tasktracker.map.tasks.maximum    = "2";
  mapred.tasktracker.reduce.tasks.maximum = "1";
c1.medium 
  mapred.tasktracker.map.tasks.maximum    = "4";
  mapred.tasktracker.reduce.tasks.maximum = "2";
m1.large 
  mapred.tasktracker.map.tasks.maximum    = "4";
  mapred.tasktracker.reduce.tasks.maximum = "2";
m1.xlarge
  mapred.tasktracker.map.tasks.maximum    = "8";
  mapred.tasktracker.reduce.tasks.maximum = "4";
c1.xlarge 
  mapred.tasktracker.map.tasks.maximum    = "8";
  mapred.tasktracker.reduce.tasks.maximum = "4";
</pre>
</div>

<p>This can be set in Whirr recipe using the following properties:</p>
<div class="highlight">
<pre><span class="na">hadoop-mapreduce.mapred.tasktracker.map.tasks.maximum</span><span class="o">=</span><span class="s">8</span>
<span class="na">hadoop-mapreduce.mapred.tasktracker.reduce.tasks.maximum</span><span class="o">=</span><span class="s">4</span>
</pre>
</div>

<p>Note that it is typically best to go with less than this amount as when a Hadoop cluster is under heavy load, JVMs can start to fail. Moreover, even with less mappers/reducers, performance is not greatly effected as there is less OS time spent dealing with swapping threads in and out of processing. Given that EC2 is a virtual machine environment, <span class="caps">CPU</span> statistics can be inaccurate. A safe configuration to use is:</p>
<div class="highlight">
<pre><span class="na">hadoop-mapreduce.mapred.tasktracker.jetty.cpu.check.enabled</span><span class="o">=</span><span class="s">false</span>
</pre>
</div>

<p>Finally, if Oracle’s Java <span class="caps">JVM</span> is desired, newer version of Whirr support the following Java install function.</p>
<div class="highlight">
<pre><span class="na">whirr.java.install-function</span><span class="o">=</span><span class="s">install_oab_java</span>
</pre>
</div>

<h2>Running a Faunus Script</h2>
<p><img src="images/faunus-elephants.png" alt=""></p>
<p>Faunus can deploy jobs to the Amazon EC2 cluster. The first thing to do is to put a graph on <span class="caps">HDFS</span>. For this example, use the toy <code>data/graph-of-the-gods.json</code> file. Once the file is in <span class="caps">HDFS</span>, a <code>faunus.sh</code> derivation can be executed.</p>
<div class="highlight">
<pre>faunus$ bin/gremlin.sh

         \,,,/
         (o o)
-----oOOo-(_)-oOOo-----
gremlin&gt; hdfs.copyFromLocal('data/graph-of-the-gods.json','graph-of-the-gods.json')
==&gt;null
gremlin&gt; g = FaunusFactory.open('bin/faunus.properties')
==&gt;faunusgraph[graphsoninputformat-&gt;graphsonoutputformat]
gremlin&gt; g.V.out.type
12/09/18 00:29:34 INFO mapreduce.FaunusCompiler: Compiled to 2 MapReduce job(s)
12/09/18 00:29:34 INFO mapreduce.FaunusCompiler: Executing job 1 out of 2: MapSequence[com.thinkaurelius.faunus.mapreduce.transform.VerticesMap.Map, com.thinkaurelius.faunus.mapreduce.transform.VerticesVerticesMapReduce.Map, com.thinkaurelius.faunus.mapreduce.transform.VerticesVerticesMapReduce.Reduce]
12/09/18 00:29:34 INFO mapreduce.FaunusCompiler: Job data location: output/job-0
...
==&gt;titan
==&gt;god
==&gt;god
==&gt;god
==&gt;god
==&gt;god
==&gt;god
==&gt;god
==&gt;location
==&gt;location
==&gt;location
==&gt;location
==&gt;human
==&gt;monster
==&gt;monster
==&gt;...
gremlin&gt; hdfs.ls('output/*') 
==&gt;rw-r--r-- ubuntu supergroup 0 _SUCCESS
==&gt;rwxr-xr-x ubuntu supergroup 0 (D) _logs
==&gt;rw-r--r-- ubuntu supergroup 0 _SUCCESS
==&gt;rwxr-xr-x ubuntu supergroup 0 (D) _logs
==&gt;rw-r--r-- ubuntu supergroup 435 graph-m-00000.bz2
==&gt;rw-r--r-- ubuntu supergroup 75 sideeffect-m-00000.bz2
</pre>
</div>

<p><img src="images/dead-elephants.png" alt=""></p>
<p>When the cluster is no longer needed, Whirr can be used to shutdown the cluster.</p>
<div class="highlight">
<pre>faunus$ whirr destroy-cluster --config bin/whirr.properties 
Starting to run scripts on cluster for phase destroyinstances: us-east-1/i-16376a6e, us-east-1/i-0c376a74, us-east-1/i-0a376a72
Running destroy phase script on: us-east-1/i-16376a6e
Running destroy phase script on: us-east-1/i-0c376a74
Starting to run scripts on cluster for phase destroyinstances: us-east-1/i-08376a70
Running destroy phase script on: us-east-1/i-0a376a72
Running destroy phase script on: us-east-1/i-08376a70
destroy phase script run completed on: us-east-1/i-0c376a74
destroy phase script run completed on: us-east-1/i-0a376a72
destroy phase script run completed on: us-east-1/i-16376a6e
Successfully executed destroy script: [output=, error=, exitCode=0]
destroy phase script run completed on: us-east-1/i-08376a70
Successfully executed destroy script: [output=, error=, exitCode=0]
Successfully executed destroy script: [output=, error=, exitCode=0]
Successfully executed destroy script: [output=, error=, exitCode=0]
Finished running destroy phase scripts on all cluster instances
Destroying faunuscluster cluster
Cluster faunuscluster destroyed
faunus$
</pre>
</div>

              </div>
            </div>
          </div>
          <div class="admin">
            <div style="float: left;">
              <small>Last edited by <b>Dan LaRocque</b>, 2013-10-16 15:36:11</small>
            </div>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
